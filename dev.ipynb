{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5536702c-3585-48e3-8069-b70afad9b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "import chromadb\n",
    "\n",
    "# from llama_index.llms import Ollama\n",
    "# from llama_index import VectorStoreIndex, ServiceContext, download_loader\n",
    "# from llama_index.storage.storage_context import StorageContext\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8dd142-0328-4812-b640-ce580c630465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load, split, and retrieve documents\n",
    "def load_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict()\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# Function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Function that defines the RAG chain\n",
    "def rag_chain(url, question):\n",
    "    retriever = load_and_retrieve_docs(url)\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\"\n",
    "    response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b919e74-315b-4bab-8759-52abe8955b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://aws.amazon.com/what-is/retrieval-augmented-generation/'\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url,),\n",
    "    bs_kwargs=dict()\n",
    ")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bebcaac-eee5-4c1b-882d-3ccf8fd49a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is RAG? - Retrieval-Augmented Generation Explained - AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Skip to main content\\n\\n\\n\\n\\n\\nClick here to return to Amazon Web Services homepage\\n\\n\\n\\nContact Us\\n Support\\xa0 \\nEnglish\\xa0\\nMy Account\\xa0\\n\\n\\n\\n\\n Sign In\\n\\n\\n  Create an AWS Account \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\nSolutions\\nPricing\\nDocumentation\\nLearn\\nPartner Network\\nAWS Marketplace\\nCustomer Enablement\\nEvents\\nExplore More \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Close \\n\\n\\n\\nعربي\\nBahasa Indonesia\\nDeutsch\\nEnglish\\nEspañol\\nFrançais\\nItaliano\\nPortuguês\\n\\n\\n\\n\\nTiếng Việt\\nTürkçe\\nΡусский\\nไทย\\n日本語\\n한국어\\n中文 (简体)\\n中文 (繁體)\\n\\n\\n\\n\\n\\n Close \\n\\nMy Profile\\nSign out of AWS Builder ID\\nAWS Management Console\\nAccount Settings\\nBilling & Cost Management\\nSecurity Credentials\\nAWS Personal Health Dashboard\\n\\n\\n\\n Close \\n\\nSupport Center\\nExpert Help\\nKnowledge Center\\nAWS Support Overview\\nAWS re:Post\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClick here to return to Amazon Web Services homepage\\n\\n\\n\\n\\n\\n\\n\\n  Get Started for Free \\n\\n\\n  Contact Us'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].dict()['page_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba1d783-2c9c-4a24-8a0b-b6da59c20112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Support Center\\nExpert Help\\nKnowledge Center\\nAWS Support Overview\\nAWS re:Post\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClick here to return to Amazon Web Services homepage\\n\\n\\n\\n\\n\\n\\n\\n  Get Started for Free \\n\\n\\n  Contact Us \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Products \\n Solutions \\n Pricing \\n Introduction to AWS \\n Getting Started \\n Documentation \\n Training and Certification \\n Developer Center \\n Customer Success \\n Partner Network \\n AWS Marketplace \\n Support \\n AWS re:Post \\n Log into Console \\n Download the Mobile App \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Cloud Computing?\\nCloud Computing Concepts Hub\\nGenerative AI\\nMachine Learning & AI\\n\\n\\nWhat Is RAG?\\n\\n\\nCreate an AWS Account\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n             Explore Free Machine Learning Offers \\n           \\n\\n             Build, deploy, and run machine learning applications in the cloud for free \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n             Check out Machine Learning Services \\n           \\n\\n             Innovate faster with the most comprehensive set of AI and ML services'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[1].dict()['page_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8de0f08-33e2-44fe-aa41-f062e2990bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "745"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits[0].dict()['page_content'])-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e22d10-ddc9-470f-8d6b-4cb20c67d5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n Close \\n\\nSupport Center\\nExpert Help\\nKnowledge Center\\nAWS Support Overview\\nAWS re:Post\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClick here to return to Amazon Web Services homepage\\n\\n\\n\\n\\n\\n\\n\\n  Get Started for Free \\n\\n\\n  Contact Us'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].dict()['page_content'][745:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a26576-1758-4eb0-b37c-2fdcf843a004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Support Center\\nExpert Help\\nKnowledge Center\\nAWS Support Overview\\nAWS re:Post\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClick here to return to Amazon Web Services homepage\\n\\n\\n\\n\\n\\n\\n\\n  Get Started for Free \\n\\n\\n  Contact Us \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[1].dict()['page_content'][0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68cf9428-6081-4ef1-8900-5db7d1c38280",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57930700-1d32-4db0-b8cb-06a35dcdf4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 156 ms\n",
      "Wall time: 49.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32cff0b3-cbaf-4f0f-8872-6b5c87590777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "new_doc = Document(\n",
    "    page_content=\"Uncle Chub is a cool guy\",\n",
    "    metadata={\n",
    "        \"source\": \"JointBoi\",\n",
    "        \"page\": 1\n",
    "    }\n",
    ")\n",
    "new_docs = [new_doc]\n",
    "new_docs_id = db.add_documents(\n",
    "    new_docs,\n",
    "    ids=[\"5555555555\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3ff57eb-7f2e-414f-ad00-65153b419c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac5059a0-3143-478b-ada8-a1f95f909246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Chroma__query_collection',\n",
       " '_LANGCHAIN_DEFAULT_COLLECTION_NAME',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_asimilarity_search_with_relevance_scores',\n",
       " '_cosine_relevance_score_fn',\n",
       " '_euclidean_relevance_score_fn',\n",
       " '_get_retriever_tags',\n",
       " '_max_inner_product_relevance_score_fn',\n",
       " '_select_relevance_score_fn',\n",
       " '_similarity_search_with_relevance_scores',\n",
       " 'aadd_documents',\n",
       " 'aadd_texts',\n",
       " 'add_documents',\n",
       " 'add_images',\n",
       " 'add_texts',\n",
       " 'adelete',\n",
       " 'afrom_documents',\n",
       " 'afrom_texts',\n",
       " 'amax_marginal_relevance_search',\n",
       " 'amax_marginal_relevance_search_by_vector',\n",
       " 'as_retriever',\n",
       " 'asearch',\n",
       " 'asimilarity_search',\n",
       " 'asimilarity_search_by_vector',\n",
       " 'asimilarity_search_with_relevance_scores',\n",
       " 'asimilarity_search_with_score',\n",
       " 'delete',\n",
       " 'delete_collection',\n",
       " 'embeddings',\n",
       " 'encode_image',\n",
       " 'from_documents',\n",
       " 'from_texts',\n",
       " 'get',\n",
       " 'max_marginal_relevance_search',\n",
       " 'max_marginal_relevance_search_by_vector',\n",
       " 'persist',\n",
       " 'search',\n",
       " 'similarity_search',\n",
       " 'similarity_search_by_vector',\n",
       " 'similarity_search_by_vector_with_relevance_scores',\n",
       " 'similarity_search_with_relevance_scores',\n",
       " 'similarity_search_with_score',\n",
       " 'update_document',\n",
       " 'update_documents']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(Chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dab38800-87b6-4907-a252-50b62e114eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how can I use rag?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fcc7e26-6caa-471b-96f9-702d32d3856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does Retrieval-Augmented Generation work?\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = db.similarity_search(query)\n",
    "print(res[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc6d10e7-6e3e-45d8-b9c2-7d1e710e2b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.vectorstores.chroma.Chroma"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fc8ccdb-aade-4b4b-83c5-7a20976325fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db2 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3df58d97-15c3-46dd-9391-2335ee5fa24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the difference between Retrieval-Augmented Generation and semantic search?\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 2.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = \"Uncle Chub?\"\n",
    "print(db2.as_retriever().invoke(query)[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f9e1e06-bd4c-4e5b-8d1e-b0ba7192442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_content': 'How does Retrieval-Augmented Generation work?',\n",
       " 'metadata': {'description': 'What is Retrieval-Augmented Generation how and why businesses use Retrieval-Augmented Generation, and how to use Retrieval-Augmented Generation with AWS.',\n",
       "  'language': 'en-US',\n",
       "  'source': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/',\n",
       "  'title': 'What is RAG? - Retrieval-Augmented Generation Explained - AWS'},\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b887b6f-f38d-47e2-b921-46ba7df7c5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='How does Retrieval-Augmented Generation work?', metadata={'description': 'What is Retrieval-Augmented Generation how and why businesses use Retrieval-Augmented Generation, and how to use Retrieval-Augmented Generation with AWS.', 'language': 'en-US', 'source': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/', 'title': 'What is RAG? - Retrieval-Augmented Generation Explained - AWS'}),\n",
       " Document(page_content='What is the difference between Retrieval-Augmented Generation and semantic search?', metadata={'description': 'What is Retrieval-Augmented Generation how and why businesses use Retrieval-Augmented Generation, and how to use Retrieval-Augmented Generation with AWS.', 'language': 'en-US', 'source': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/', 'title': 'What is RAG? - Retrieval-Augmented Generation Explained - AWS'}),\n",
       " Document(page_content='You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\\nRAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\\n\\n\\n\\n\\n\\n\\n\\nWhat are the benefits of Retrieval-Augmented Generation?', metadata={'description': 'What is Retrieval-Augmented Generation how and why businesses use Retrieval-Augmented Generation, and how to use Retrieval-Augmented Generation with AWS.', 'language': 'en-US', 'source': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/', 'title': 'What is RAG? - Retrieval-Augmented Generation Explained - AWS'}),\n",
       " Document(page_content=\"What is Retrieval-Augmented Generation?\\n\\nRetrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\\n\\n\\n\\n\\n\\n\\n\\nWhy is Retrieval-Augmented Generation important?\", metadata={'description': 'What is Retrieval-Augmented Generation how and why businesses use Retrieval-Augmented Generation, and how to use Retrieval-Augmented Generation with AWS.', 'language': 'en-US', 'source': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/', 'title': 'What is RAG? - Retrieval-Augmented Generation Explained - AWS'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.as_retriever().invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d362998b-87e6-4995-95f7-d7db350db1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02b17f26-1a79-4abb-9bb4-063a132d4693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the difference between Retrieval-Augmented Generation and semantic search?\n"
     ]
    }
   ],
   "source": [
    "query = 'what is rag?'\n",
    "res = vectorstore.similarity_search(query)\n",
    "print(res[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a3fcaaa-35a5-4d2a-a180-5b6416c59c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'what is rag?'\n",
    "# vectorstore.as_retriever().invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "696f99ac-b9bf-4168-ad14-8b75639195d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = load_and_retrieve_docs(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db89a7fa-5519-4a0d-8ae4-d42f5828aff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\\nRAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\\n\\n\\n\\n\\n\\n\\n\\nWhat are the benefits of Retrieval-Augmented Generation?\\n\\nYou can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\\nRAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\\n\\n\\n\\n\\n\\n\\n\\nWhat are the benefits of Retrieval-Augmented Generation?\\n\\nRetrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\\nUse pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\\nSupport a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\\nFilter responses based on those documents that the end-user permissions allow.\\n\\nAmazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\\nGet started with Retrieval-Augmented Generation on AWS by creating a free account today\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Next Steps on AWS\\n\\nRetrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\\nUse pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\\nSupport a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\\nFilter responses based on those documents that the end-user permissions allow.\\n\\nAmazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\\nGet started with Retrieval-Augmented Generation on AWS by creating a free account today\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Next Steps on AWS'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'How can I use rag'\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "formatted_context = format_docs(retrieved_docs)\n",
    "formatted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b44b445-76ca-4869-85ac-e7560f9e618e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\\nRAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.\\n\\n\\n\\n\\n\\n\\n\\nWhat are the benefits of Retrieval-Augmented Generation?', metadata={'description': 'What is Retrieval-Augmented Generation how and why businesses use Retrieval-Augmented Generation, and how to use Retrieval-Augmented Generation with AWS.', 'language': 'en-US', 'source': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/', 'title': 'What is RAG? - Retrieval-Augmented Generation Explained - AWS'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83869590-bb36-4c4b-9f8f-1813fa0379ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcb7e777-a65e-45b4-9143-681674bc1f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doublebank\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79.3M/79.3M [00:10<00:00, 8.23MiB/s]\n"
     ]
    }
   ],
   "source": [
    "collection.add(\n",
    "    documents=[\"Here is a book\", \"A book is here\", \"Here is a fucking book\", \"Here is a book!!\"],\n",
    "    metadatas=[\n",
    "        {\"source\": \"polite book\", \"page\":1},\n",
    "        {\"source\": \"sophisicated book\", \"page\":7},\n",
    "        {\"source\": \"bad mouth\", \"page\":9},\n",
    "        {\"source\": \"cult movit\", \"script\": 10},\n",
    "    ],\n",
    "    ids=[\"id1\", \"id2\", \"id3\", \"id4\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb3fa5c9-5408-40b7-8b4b-19783843f37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id3', 'id2']],\n",
       " 'distances': [[0.6216685771942139, 0.6718968749046326]],\n",
       " 'metadatas': [[{'page': 9, 'source': 'bad mouth'},\n",
       "   {'page': 7, 'source': 'sophisicated book'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Here is a fucking book', 'A book is here']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_texts = [\"Where is my fucking book?\"]\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=query_texts,\n",
    "    n_results=2,\n",
    "    include=[\n",
    "        \"distances\",\n",
    "        \"metadatas\",\n",
    "        # \"embeddings\",\n",
    "        \"documents\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1710c252-74ad-4df1-ac15-b0368427e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_with_source(results):\n",
    "    idx = results['distances'][0].index(min(results['distances'][0]))\n",
    "    return {\n",
    "        'ids': results['ids'][0][idx],\n",
    "        'distances': results['distances'][0][idx],\n",
    "        'metadatas': results['metadatas'][0][idx],\n",
    "        'documents': results['documents'][0][idx]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc6a0cdb-b5e7-4d67-992c-d491bf187ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': 'id3',\n",
       " 'distances': 0.6216685771942139,\n",
       " 'metadatas': {'page': 9, 'source': 'bad mouth'},\n",
       " 'documents': 'Here is a fucking book'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_with_source(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ba1c4-d739-4cdc-8ea2-a479064622fe",
   "metadata": {},
   "source": [
    "# What do we need to do  \n",
    "- Understand how text extraction work: url, pdf  \n",
    "- Understand how langchain works  \n",
    "- Understand how to store data into vector database  \n",
    "- Understand how to retrieve similar vectors from vector database  \n",
    "- Understand how to generate the prompt  \n",
    "- Understand how to fetch the prompt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa958af8-f2c2-4f44-9dd3-e484ae0d410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://aws.amazon.com/what-is/retrieval-augmented-generation/'\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url,),\n",
    "    bs_kwargs=dict()\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec3ad763-9dd0-4906-904b-55f58e8abf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11836d2f-5785-4ae6-863b-8cca45c56396",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d89cfcc-67c9-49e1-8c5b-69e5bc2b225b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Chroma__query_collection',\n",
       " '_LANGCHAIN_DEFAULT_COLLECTION_NAME',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_asimilarity_search_with_relevance_scores',\n",
       " '_client',\n",
       " '_client_settings',\n",
       " '_collection',\n",
       " '_cosine_relevance_score_fn',\n",
       " '_embedding_function',\n",
       " '_euclidean_relevance_score_fn',\n",
       " '_get_retriever_tags',\n",
       " '_max_inner_product_relevance_score_fn',\n",
       " '_persist_directory',\n",
       " '_select_relevance_score_fn',\n",
       " '_similarity_search_with_relevance_scores',\n",
       " 'aadd_documents',\n",
       " 'aadd_texts',\n",
       " 'add_documents',\n",
       " 'add_images',\n",
       " 'add_texts',\n",
       " 'adelete',\n",
       " 'afrom_documents',\n",
       " 'afrom_texts',\n",
       " 'amax_marginal_relevance_search',\n",
       " 'amax_marginal_relevance_search_by_vector',\n",
       " 'as_retriever',\n",
       " 'asearch',\n",
       " 'asimilarity_search',\n",
       " 'asimilarity_search_by_vector',\n",
       " 'asimilarity_search_with_relevance_scores',\n",
       " 'asimilarity_search_with_score',\n",
       " 'delete',\n",
       " 'delete_collection',\n",
       " 'embeddings',\n",
       " 'encode_image',\n",
       " 'from_documents',\n",
       " 'from_texts',\n",
       " 'get',\n",
       " 'max_marginal_relevance_search',\n",
       " 'max_marginal_relevance_search_by_vector',\n",
       " 'override_relevance_score_fn',\n",
       " 'persist',\n",
       " 'search',\n",
       " 'similarity_search',\n",
       " 'similarity_search_by_vector',\n",
       " 'similarity_search_by_vector_with_relevance_scores',\n",
       " 'similarity_search_with_relevance_scores',\n",
       " 'similarity_search_with_score',\n",
       " 'update_document',\n",
       " 'update_documents']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6045c398-2b5d-4e73-abea-6f847e8f09c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3fb5671a-68ab-4796-b3ca-57b502f7657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I use rag on AWS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e54c5e28-b73f-4716-80f7-1a07ed4794e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77c73746-eb49-4374-b2ad-abdaeabfc65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0887e024-8926-435e-9689-11667052c6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_content': 'Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\\nUse pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\\nSupport a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\\nFilter responses based on those documents that the end-user permissions allow.\\n\\nAmazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\\nGet started with Retrieval-Augmented Generation on AWS by creating a free account today\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Next Steps on AWS',\n",
       " 'metadata': {'description': 'What is Retrieval-Augmented Generation how and why businesses use Retrieval-Augmented Generation, and how to use Retrieval-Augmented Generation with AWS.',\n",
       "  'language': 'en-US',\n",
       "  'source': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/',\n",
       "  'title': 'What is RAG? - Retrieval-Augmented Generation Explained - AWS'},\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87ec3080-82e1-493a-a9ce-6f7dddebb3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_content': 'Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\\nUse pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\\nSupport a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\\nFilter responses based on those documents that the end-user permissions allow.\\n\\nAmazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\\nGet started with Retrieval-Augmented Generation on AWS by creating a free account today\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Next Steps on AWS',\n",
       " 'metadata': {'description': 'What is Retrieval-Augmented Generation how and why businesses use Retrieval-Augmented Generation, and how to use Retrieval-Augmented Generation with AWS.',\n",
       "  'language': 'en-US',\n",
       "  'source': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/',\n",
       "  'title': 'What is RAG? - Retrieval-Augmented Generation Explained - AWS'},\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[1].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74ad0457-aa5f-4a18-8b25-6313977f5b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_content': 'Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\\nUse pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\\nSupport a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\\nFilter responses based on those documents that the end-user permissions allow.\\n\\nAmazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\\nGet started with Retrieval-Augmented Generation on AWS by creating a free account today\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Next Steps on AWS',\n",
       " 'metadata': {'description': 'What is Retrieval-Augmented Generation how and why businesses use Retrieval-Augmented Generation, and how to use Retrieval-Augmented Generation with AWS.',\n",
       "  'language': 'en-US',\n",
       "  'source': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/',\n",
       "  'title': 'What is RAG? - Retrieval-Augmented Generation Explained - AWS'},\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[2].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cff456a2-8f1f-411d-bb51-aadb65c5f150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_content': 'Retrieve relevant information\\nThe next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee\\'s past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations.\\nAugment the LLM prompt\\nNext, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries.\\nUpdate external data',\n",
       " 'metadata': {'description': 'What is Retrieval-Augmented Generation how and why businesses use Retrieval-Augmented Generation, and how to use Retrieval-Augmented Generation with AWS.',\n",
       "  'language': 'en-US',\n",
       "  'source': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/',\n",
       "  'title': 'What is RAG? - Retrieval-Augmented Generation Explained - AWS'},\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[3].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "93882f50-eeaf-4485-85f7-5f93c64a6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_context = format_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec342b53-2c07-4c65-9176-e3525f71230b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3632"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(formatted_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe962d44-058e-4b8d-a8d4-81120f7e7f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\\nUse pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\\nSupport a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\\nFilter responses based on those documents that the end-user permissions allow.\\n\\nAmazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\\nGet started with Retrieval-Augmented Generation on AWS by creating a free account today\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Next Steps on AWS\\n\\nRetrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\\nUse pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\\nSupport a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\\nFilter responses based on those documents that the end-user permissions allow.\\n\\nAmazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\\nGet started with Retrieval-Augmented Generation on AWS by creating a free account today\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Next Steps on AWS\\n\\nRetrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\\nUse pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\\nSupport a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\\nFilter responses based on those documents that the end-user permissions allow.\\n\\nAmazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\\nGet started with Retrieval-Augmented Generation on AWS by creating a free account today\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Next Steps on AWS\\n\\nRetrieve relevant information\\nThe next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee\\'s past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations.\\nAugment the LLM prompt\\nNext, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries.\\nUpdate external data'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ae90ce63-3242-4166-89b1-e86b97159b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "29424031-b423-495d-b2b9-6ac0ec69760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can I use rag on AWS\n",
      "\n",
      "Context: Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\n",
      "Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\n",
      "Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\n",
      "Filter responses based on those documents that the end-user permissions allow.\n",
      "\n",
      "Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\n",
      "Get started with Retrieval-Augmented Generation on AWS by creating a free account today\n",
      "\n",
      "\n",
      " Next Steps on AWS\n",
      "\n",
      "Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\n",
      "Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\n",
      "Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\n",
      "Filter responses based on those documents that the end-user permissions allow.\n",
      "\n",
      "Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\n",
      "Get started with Retrieval-Augmented Generation on AWS by creating a free account today\n",
      "\n",
      "\n",
      " Next Steps on AWS\n",
      "\n",
      "Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\n",
      "Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\n",
      "Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\n",
      "Filter responses based on those documents that the end-user permissions allow.\n",
      "\n",
      "Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\n",
      "Get started with Retrieval-Augmented Generation on AWS by creating a free account today\n",
      "\n",
      "\n",
      " Next Steps on AWS\n",
      "\n",
      "Retrieve relevant information\n",
      "The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee's past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations.\n",
      "Augment the LLM prompt\n",
      "Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries.\n",
      "Update external data\n"
     ]
    }
   ],
   "source": [
    "print(formatted_prompt.replace(\"\\n\\n\\n\\n\\n\", '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a48a779-15e1-4683-8007-68bc15143f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can I use rag on AWS\n",
      "\n",
      "Context: Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\n",
      "Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\n",
      "Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\n",
      "Filter responses based on those documents that the end-user permissions allow.\n",
      "\n",
      "Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\n",
      "Get started with Retrieval-Augmented Generation on AWS by creating a free account today\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Next Steps on AWS\n",
      "\n",
      "Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\n",
      "Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\n",
      "Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\n",
      "Filter responses based on those documents that the end-user permissions allow.\n",
      "\n",
      "Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\n",
      "Get started with Retrieval-Augmented Generation on AWS by creating a free account today\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Next Steps on AWS\n",
      "\n",
      "Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance.\n",
      "Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites.\n",
      "Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files.\n",
      "Filter responses based on those documents that the end-user permissions allow.\n",
      "\n",
      "Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.\n",
      "Get started with Retrieval-Augmented Generation on AWS by creating a free account today\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Next Steps on AWS\n",
      "\n",
      "Retrieve relevant information\n",
      "The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee's past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations.\n",
      "Augment the LLM prompt\n",
      "Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries.\n",
      "Update external data\n"
     ]
    }
   ],
   "source": [
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3574a-843c-412f-9aaf-980b07640e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(question)\n",
    "formatted_context = format_docs(retrieved_docs)\n",
    "formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\"\n",
    "response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d4a0a-98cd-4ebc-aeb5-11ddbe26b6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "726e3c17-8b6e-4ab9-8762-730e45b9c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt.replace(\"\\n\\n\\n\\n\\n\", '\\n')}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f51101d1-b1f0-40c3-87a3-5e343e40d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To use Retrieval-Augmented Generation (RAG) on AWS, follow these next steps:\n",
      "\n",
      "1. **Set up your environment:** Get started by creating a free Amazon Web Services (AWS) account if you don't already have one. You can sign up at https://aws.amazon.com. After signing up, familiarize yourself with the AWS Management Console and choose the services you'll need for RAG implementation - Amazon SageMaker, Amazon Simple Storage Service (S3), and other relevant services.\n",
      "\n",
      "2. **Store your data:** Use Amazon S3 to store your documents in a centralized location that can be accessed by RAG models. Make sure your data is indexed using Amazon SageMaker's search service, such as Amazon Elasticsearch or Amazon OpenSearch Service, for efficient retrieval.\n",
      "\n",
      "3. **Retrieve relevant information:** Perform relevancy searches based on user queries by converting the query into a vector representation and matching it with your vector databases. Use pre-built connectors to access data from popular technologies like Amazon S3, SharePoint, Confluence, and other websites. Make sure to filter responses based on end-user permissions as needed.\n",
      "\n",
      "4. **Augment the LLM prompt:** Once you have retrieved relevant data, augment the user input (or prompts) by adding the retrieved data in context using prompt engineering techniques. This step will allow large language models like Amazon SageMaker's fine-tuned models to generate accurate answers to user queries based on the additional information provided.\n",
      "\n",
      "5. **Update external data:** If necessary, update your external data sources (such as SharePoint or Confluence) with the retrieved and augmented responses for further use. This step will ensure that your data remains up-to-date and available for future searches and RAG applications.\n",
      "\n",
      "6. **Deploy and scale:** Use Amazon SageMaker to deploy and scale your RAG solution as needed. You can also refer to the existing SageMaker notebooks and code examples provided by AWS to speed up implementation.\n",
      "\n",
      "Get started today and discover how Retrieval-Augmented Generation on AWS can help you retrieve semantically-relevant passages of text, order them by relevance, use pre-built connectors for popular data technologies, support a wide range of document formats, and filter responses based on end-user permissions.\n"
     ]
    }
   ],
   "source": [
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac2515e7-ea71-4d8a-9f25-473c053c8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "79458854-0699-4bd6-96f4-c10af13759f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To use RAG (Retrieval-Augmented Generation) on AWS, follow these steps:\n",
      "\n",
      "1. Create an AWS account: If you don't have one already, sign up for a free AWS account at https://aws.amazon.com/.\n",
      "2. Set up necessary services: Install and configure the Amazon SageMaker notebook instance with the required dependencies, such as RAG library and data connectors (SharePoint, Confluence, etc.). You can use Amazon SageMaker JumpStart to get started quickly.\n",
      "3. Prepare your data: Store your document datasets in AWS services like Amazon Simple Storage Service (S3), SharePoint, or Confluence, which RAG can connect to directly. Ensure proper access controls are in place for the documents.\n",
      "4. Perform relevancy search: Use RAG model to convert user queries into vector representations and search for relevant passages from your document datasets using pre-built connectors and filtering based on end-user permissions. You may use existing SageMaker notebooks or code examples as a starting point.\n",
      "5. Augment the LLM prompt: Use prompt engineering techniques to add the retrieved data in context to augment the user input for the large language model (LLM). This enables the LLM to generate accurate answers based on the input and contextual information.\n",
      "6. Update external data: Keep your document datasets up-to-date, ensuring that they remain relevant for the RAG system's performance. Regularly update your AWS services with new or modified documents as needed.\n",
      "7. Monitor and iterate: Continuously evaluate and improve the RAG model's performance by monitoring user feedback and analytics, and making necessary adjustments to enhance the system's accuracy and efficiency.\n"
     ]
    }
   ],
   "source": [
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a3b32243-e6d0-4d2f-a5b6-196b2f1888ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It seems like you are describing how to use Retrieval-Augmented Generation (RAG) on Amazon Web Services (AWS). Here's a more detailed explanation of the next steps after retrieving semantically-relevant passages using RAG on AWS:\n",
      "\n",
      "1. Relevancy search: The user query is converted into a vector representation and matched with the vector databases. This process involves calculating the relevance between the user query and the documents in the database using mathematical vector calculations and representations. The documents that have the highest relevance scores are retrieved and considered semantically relevant to the user query.\n",
      "2. Augment the LLM prompt: In this step, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This is done using prompt engineering techniques to effectively communicate with the Large Language Model (LLM). The augmented prompt allows the LLM to generate an accurate answer to user queries based on the relevant information retrieved earlier.\n",
      "3. Update external data: If necessary, you may also need to update external data sources like Amazon Simple Storage Service (S3), SharePoint, Confluence, or other websites, with the results obtained from the RAG process. This step ensures that the latest and most accurate information is available for future queries.\n",
      "4. Build custom generative AI solutions: For organizations seeking more customized AI solutions, Amazon SageMaker JumpStart offers a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that can be deployed easily. These resources allow you to speed up the RAG implementation process by referring to existing SageMaker notebooks and code examples.\n",
      "5. Get started: To begin using Retrieval-Augmented Generation on AWS, create a free account today. Once you have access to the platform, follow the guidelines provided to set up your environment, install necessary dependencies, and configure your data sources. From there, you can start experimenting with RAG to retrieve relevant information for various user queries.\n",
      "\n",
      "By following these steps, you will be able to effectively use Retrieval-Augmented Generation on AWS to retrieve semantically-relevant passages, support a wide range of document formats, filter responses based on end-user permissions, and augment language model prompts with the retrieved data.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt.replace(\"\\n\\n\\n\\n\\n\", '\\n')}])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "97caeb4a-4b7d-45cf-936e-9bfc3b1f5419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It seems that you are describing how to use Retrieval-Augmented Generation (RAG) on Amazon Web Services (AWS), specifically for information retrieval and augmenting language model prompts with relevant data. Here's a step-by-step guide based on the provided context:\n",
      "\n",
      "1. Set up an AWS account: If you don't have one already, sign up for a free AWS account to get started. You can create an account at https://aws.amazon.com/.\n",
      "\n",
      "2. Use RAG connectors: RAG supports pre-built connectors to popular data technologies such as Amazon Simple Storage Service (S3), SharePoint, Confluence, and other websites. Configure these connectors to access your data sources.\n",
      "\n",
      "3. Upload documents: Upload the required document formats (HTML, Word, PowerPoint, PDF, Excel, and text files) into AWS S3 or other supported storage services.\n",
      "\n",
      "4. Retrieve semantically-relevant passages: Use RAG to search and retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance. Filter responses based on documents that the end-user permissions allow.\n",
      "\n",
      "5. Perform relevancy search: Convert user queries into vector representation and match them with vector databases to establish relevancy. Use mathematical vector calculations and representations for this step.\n",
      "\n",
      "6. Augment LLM prompt: Augment the language model prompt (LLM) with the relevant retrieved data in context using prompt engineering techniques. This allows effective communication between the user and LLM.\n",
      "\n",
      "7. Update external data (optional): Depending on your use case, you might need to update external data periodically. RAG offers various options for organizations who want to build more custom generative AI solutions. Use Amazon SageMaker JumpStart or refer to existing SageMaker notebooks and code examples for further implementation assistance.\n",
      "\n",
      "Follow these steps to get started with Retrieval-Augmented Generation on AWS.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='mistral', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a2ec888-f51f-4a16-9850-e59f6ad81308",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9551b5-06a1-4034-8daf-01756d4f7fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
